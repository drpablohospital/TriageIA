import pandas as pd
import tkinter as tk
from tkinter import filedialog
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch
import os
import time

# Configuraci√≥n inicial con m√°s detalles
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"‚öôÔ∏è Dispositivo seleccionado: {device}")
print(f"‚öôÔ∏è Versi√≥n de PyTorch: {torch.__version__}")
print(f"‚öôÔ∏è Transformers disponible: {'transformers' in globals()}")

def seleccionar_archivo():
    try:
        root = tk.Tk()
        root.withdraw()
        archivo = filedialog.askopenfilename(
            title="Selecciona el archivo CSV con la muestra", 
            filetypes=[("CSV files", "*.csv")]
        )
        if archivo:
            print(f"üìÅ Archivo seleccionado: {archivo}")
            if not os.path.exists(archivo):
                print("‚ùå El archivo no existe en la ruta especificada")
                return None
            if os.path.getsize(archivo) == 0:
                print("‚ùå El archivo est√° vac√≠o")
                return None
        return archivo
    except Exception as e:
        print(f"‚ùå Error al seleccionar archivo: {str(e)}")
        return None

def cargar_modelo(ruta_modelo):
    """Carga un modelo y su tokenizer desde la ruta especificada con validaciones"""
    print(f"\nüîç Intentando cargar modelo desde: {ruta_modelo}")
    
    if not os.path.exists(ruta_modelo):
        raise Exception(f"La ruta del modelo no existe: {ruta_modelo}")
    
    try:
        print("‚è≥ Cargando tokenizer...")
        inicio = time.time()
        tokenizer = AutoTokenizer.from_pretrained(ruta_modelo)
        print(f"‚úÖ Tokenizer cargado en {time.time() - inicio:.2f}s")
        
        print("‚è≥ Cargando modelo...")
        inicio = time.time()
        model = AutoModelForSeq2SeqLM.from_pretrained(ruta_modelo).to(device)
        print(f"‚úÖ Modelo cargado en {time.time() - inicio:.2f}s")
        
        # Verificaci√≥n r√°pida del modelo
        print("üß™ Realizando prueba r√°pida del modelo...")
        try:
            test_input = tokenizer("Prueba", return_tensors="pt").to(device)
            _ = model.generate(**test_input, max_length=5)
            print("‚úÖ Prueba de modelo exitosa")
        except Exception as e:
            print(f"‚ö†Ô∏è Advertencia en prueba de modelo: {str(e)}")
        
        return tokenizer, model
    except Exception as e:
        raise Exception(f"Error al cargar el modelo en {ruta_modelo}: {str(e)}")

def verificar_dataframe(df):
    """Verifica que el DataFrame tenga las columnas necesarias"""
    print("\nüîç Verificando estructura del DataFrame...")
    columnas_requeridas = {
        'MOTIVO': str,
        'EDAD': (int, float),
        'GENERO': str,
        'FC': (int, float),
        'FR': (int, float),
        'TAS': (int, float),
        'SAO2': (int, float),
        'TEMP': (int, float),
        'NEWS2': (int, float)
    }
    
    faltantes = [col for col in columnas_requeridas if col not in df.columns]
    if faltantes:
        raise ValueError(f"Columnas faltantes: {faltantes}")
    
    print("‚úÖ Estructura b√°sica del DataFrame es correcta")
    
    # Verificar muestras de datos
    print("\nüìù Muestra de datos (primer registro):")
    primer_registro = df.iloc[0]
    for col in columnas_requeridas:
        print(f"{col}: {primer_registro[col]} (tipo: {type(primer_registro[col])})")
    
    return True

def preparar_texto_entrada(fila):
    """Genera el texto de entrada para el modelo a partir de una fila del DataFrame"""
    try:
        texto = (
            f"Motivo: {fila['MOTIVO']} | "
            f"Edad: {fila['EDAD']} | "
            f"Genero: {fila['GENERO']} | "
            f"FC: {fila['FC']} | "
            f"FR: {fila['FR']} | "
            f"TAS: {fila['TAS']} | "
            f"SAO2: {fila['SAO2']} | "
            f"TEMP: {fila['TEMP']} | "
            f"NEWS2: {fila['NEWS2']}"
        )
        print("\nüìÑ Texto de entrada generado:")
        print(texto)
        return texto
    except Exception as e:
        raise ValueError(f"Error al preparar texto de entrada: {str(e)}")

def predecir_con_modelo(texto, tokenizer, model, max_length=32):
    """Realiza una predicci√≥n con el modelo especificado con logging detallado"""
    try:
        print("\nüîÆ Comenzando predicci√≥n...")
        inicio = time.time()
        
        print("‚è≥ Tokenizando entrada...")
        inputs = tokenizer(texto, return_tensors="pt", max_length=512, truncation=True).to(device)
        print(f"‚úÖ Tokenizaci√≥n completada en {time.time() - inicio:.2f}s")
        
        print("‚è≥ Generando predicci√≥n...")
        inicio_gen = time.time()
        outputs = model.generate(**inputs, max_length=max_length)
        print(f"‚úÖ Predicci√≥n generada en {time.time() - inicio_gen:.2f}s")
        
        resultado = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"üìå Resultado de la predicci√≥n: '{resultado}'")
        
        return resultado
    except Exception as e:
        raise Exception(f"Error durante la predicci√≥n: {str(e)}")

def main():
    print("\nüîÆ Script de Inferencia para Modelos de Triage - Versi√≥n Debug")
    print("="*70)
    
    # 1. Seleccionar archivo con la muestra
    print("\nüîÑ PASO 1: Seleccionar archivo CSV")
    archivo_muestra = seleccionar_archivo()
    if not archivo_muestra:
        print("‚ùå No se seleccion√≥ ning√∫n archivo v√°lido")
        return
    
    try:
        # 2. Cargar la muestra
        print("\nüîÑ PASO 2: Cargar y validar datos")
        print(f"‚è≥ Cargando archivo {archivo_muestra}...")
        df = pd.read_csv(archivo_muestra, encoding='utf-8')
        print(f"‚úÖ Datos cargados: {len(df)} registros, {len(df.columns)} columnas")
        
        verificar_dataframe(df)
        
        # 3. Cargar modelos entrenados
        print("\nüîÑ PASO 3: Cargar modelos entrenados")
        modelos_a_cargar = {
            'PRIORIDAD': "./modelo_t5_prioridad",
            'DERIVACION': "./modelo_t5_derivacion",
            'ESPECIALIDAD': "./modelo_t5_especialidad",
            'IDX': "./modelo_t5_idx"
        }
        
        modelos = {}
        for nombre, ruta in modelos_a_cargar.items():
            print(f"\nüîç Verificando modelo {nombre}...")
            if not os.path.exists(ruta):
                print(f"‚ùå No se encontr√≥ el modelo {nombre} en {ruta}")
                print("‚ÑπÔ∏è Contenido del directorio:")
                print(os.listdir(os.path.dirname(ruta)))
                continue
            
            try:
                modelos[nombre] = cargar_modelo(ruta)
                print(f"‚úÖ Modelo {nombre} cargado correctamente")
            except Exception as e:
                print(f"‚ùå Error al cargar modelo {nombre}: {str(e)}")
                continue
        
        if not modelos:
            raise Exception("No se pudo cargar ning√∫n modelo")
        
        # 4. Realizar predicciones para cada registro
        print("\nüîÑ PASO 4: Realizar predicciones")
        resultados = []
        total_registros = len(df)
        
        for i, (_, fila) in enumerate(df.iterrows(), 1):
            print(f"\nüìä Procesando registro {i}/{total_registros}")
            try:
                # Preparar texto de entrada
                texto_entrada = preparar_texto_entrada(fila)
                
                # Realizar predicciones solo con modelos cargados correctamente
                preds = {}
                for nombre in modelos:
                    try:
                        print(f"\nüîÆ Prediciendo {nombre}...")
                        inicio = time.time()
                        tokenizer, model = modelos[nombre]
                        preds[f'IA_{nombre}'] = predecir_con_modelo(texto_entrada, tokenizer, model)
                        print(f"‚úÖ Predicci√≥n {nombre} completada en {time.time() - inicio:.2f}s")
                    except Exception as e:
                        print(f"‚ùå Error al predecir {nombre}: {str(e)}")
                        preds[f'IA_{nombre}'] = "ERROR"
                
                resultados.append(preds)
                
            except Exception as e:
                print(f"‚ùå Error procesando registro {i}: {str(e)}")
                resultados.append({f'IA_{k}': "ERROR" for k in modelos})
        
        # 5. Agregar predicciones al DataFrame original
        print("\nüîÑ PASO 5: Consolidar resultados")
        df_resultados = pd.concat([df, pd.DataFrame(resultados)], axis=1)
        
        # 6. Guardar resultados
        print("\nüîÑ PASO 6: Guardar resultados")
        nombre_base = os.path.basename(archivo_muestra)
        nombre_resultado = f"resultados_IA_{nombre_base}"
        ruta_guardado = os.path.join(os.path.dirname(archivo_muestra), nombre_resultado)
        
        print(f"‚è≥ Guardando resultados en {ruta_guardado}...")
        df_resultados.to_csv(ruta_guardado, index=False, encoding='utf-8-sig')
        print(f"‚úÖ Resultados guardados exitosamente")
        print(f"üìä Resumen:\n{df_resultados.head()}")
        
    except Exception as e:
        print(f"\n‚ùå‚ùå‚ùå Error cr√≠tico durante el proceso: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    inicio_total = time.time()
    main()
    print(f"\nüïí Tiempo total de ejecuci√≥n: {time.time() - inicio_total:.2f} segundos")